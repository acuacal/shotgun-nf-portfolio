\
// conf/hybrid.config
// Basic configuration for AWS Batch execution
// Users should customize this based on their AWS environment

params {
    aws_region = 'your-aws-region' // Specify your AWS region
    aws_queue = 'your-aws-batch-queue' // Specify your AWS Batch Job Queue
    aws_cli_path = '/usr/local/bin/aws' // Often found here in default AWS Linux AMIs
}

process {
    executor = 'awsbatch'
    queue = params.aws_queue
    container = 'nfcore/base:1.13.3' // Default container, can be overridden by pipeline
}

aws {
    region = params.aws_region
    batch {
        cliPath = params.aws_cli_path
        jobRole = 'arn:aws:iam::xxxxxxxxxxxx:role/your-batch-job-role' // Optional: if your EC2 instances need a specific IAM role
        // validBuckets = 's3://your-s3-bucket-for-nextflow-data' // Important: specify S3 bucket for Nextflow to use
    }
}

docker {
    enabled = true
    awsBatch {
        enabled = true
    }
}

// It's good practice to explicitly enable Singularity if you might use it,
// though for pure AWS Batch with Docker, it might not be strictly necessary.
// singularity.enabled = true

// Ensure that the work directory is on a shared file system accessible by Batch jobs,
// or use S3 for the work directory.
// workDir = 's3://your-s3-bucket-for-nextflow-data/work' // Example for S3 work directory 